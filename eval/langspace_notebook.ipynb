{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from taskography_api.taskography.utils.loader import loader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import importlib\n",
    "\n",
    "#from langspace_core import *\n",
    "\n",
    "import importlib\n",
    "from langspace_core_alt import *\n",
    "from langspace_utils import *\n",
    "from langspace_planners import *\n",
    "from langspace_plot import *\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenegraph_path = '/home/laszlo/Stanford/3dscenegraph/tiny/verified_graph/3DSceneGraph_Benevolence.npz'\n",
    "building = loader(scenegraph_path)\n",
    "scenegraph_original = LSSceneGraph(building)\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "\n",
    "json_scenegraph = scenegraph.get_scene_graph()\n",
    "\n",
    "#print(json.dumps(json_scenegraph, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(json.dumps(scenegraph.get_scene_graph(), indent=4))\n",
    "\n",
    "#plot_scene_graph(scenegraph.get_scene_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenegraph2 = LSSceneGraph.from_json(json_scenegraph)\n",
    "\n",
    "\n",
    "plot_scene_graph(scenegraph2.get_scene_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "# Use the local model directory instead of Hugging Face model name\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# Load tokenizer from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model from the local directory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Reduce memory usage\n",
    "    device_map=\"auto\"  # Automatically assigns to GPU using `accelerate`\n",
    ")\n",
    "\n",
    "# Create a Hugging Face pipeline with deterministic generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=False, \n",
    "    return_full_text=False,\n",
    "    max_new_tokens=10000\n",
    "    )\n",
    "\n",
    "# Wrap pipeline in LangChain's HuggingFacePipeline\n",
    "llm_llama = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/SFT/llama3_2_3B/lora/epoch_1\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model and force to GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map={\"\": 0}  # Force GPU\n",
    ")\n",
    "\n",
    "# Confirm model is on CUDA\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Prepare input\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Ensure inputs are on CUDA\n",
    "print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "# Use the local model directory instead of Hugging Face model name\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/SFT/llama3_2_3B/lora/epoch_1\"\n",
    "\n",
    "# Load tokenizer from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model and explicitly set it to GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Reduce memory usage\n",
    "    device_map={\"\": 0}  # Automatically assigns to GPU using `accelerate`\n",
    ")\n",
    "\n",
    "# Confirm model is on CUDA\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Create a Hugging Face pipeline with deterministic generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=False, \n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "# Wrap pipeline in LangChain's HuggingFacePipeline\n",
    "llm_llama = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Test inference\n",
    "prompt = \"Hello, how are you?\"\n",
    "result = llm_llama.invoke(prompt)\n",
    "print(\"Generated Output:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# single object placement task generation\n",
    "\n",
    "system_prompt_single_placement_concrete = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. Your task is to generate natural language problem descriptions that involve moving objects between rooms and placing them on specific assets. \\\n",
    "\n",
    "Instructions: \\\n",
    "1. Based on the given 3D scene graph, generate 10 distinct task descriptions that involve: \\\n",
    "   - Picking up an object from one room. \\\n",
    "   - Carrying it to another room. \\\n",
    "   - Placing it on an asset (e.g., table, fridge, chair, bed). \\\n",
    "\n",
    "2. Keep the language natural and friendly, while ensuring clarity by explicitly specifying: \\\n",
    "   - Room identifiers and classes. \\\n",
    "   - Object identifiers and classes. \\\n",
    "   - Asset identifiers and classes. \\\n",
    "\n",
    "3. For each task description, generate a verifying step that ensures the object has been placed on the correct asset. Use the following structured format: \\\n",
    "   - Task Description Example:  \\\n",
    "     \"Please go to r_5 bedroom, pick up o_9 book, and place it on a_36 dining table in r_9 dining room.\" \\\n",
    "   - Verifying Step Format:  \\\n",
    "     \"check_object_at(<object_id>, <asset_id>)\" \\\n",
    "\n",
    "Output Format: \\\n",
    "Return the 10 generated tasks in JSON format as follows: \\\n",
    "[ \\\n",
    "  { \"instruction\": \"natural problem description 1\", \"verifying_step\": [\"check_object_at(<object_id>, <asset_id>)\"] }, \\\n",
    "  { \"instruction\": \"natural problem description 2\", \"verifying_step\": [\"check_object_at(<object_id>, <asset_id>)\"] }, \\\n",
    "  ... \\\n",
    "  { \"instruction\": \"natural problem description 10\", \"verifying_step\": \"check_object_at(<object_id>, <asset_id>)\" } \\\n",
    "] \\\n",
    "Ensure that <object_id> and <asset_id> match the corresponding values in each task description. \\\n",
    "### **Rules:** \\\n",
    "1. **Return only valid JSON.** Do **not** use markdown, `json` code blocks (` ```json ... ``` `), or any additional formatting. \\\n",
    "2. **Do not include explanations, extra text, or comments.** Only return the raw JSON. \\\n",
    "3. **Your response must start with `[` and end with `]`, and be valid JSON syntax.** \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_single_placement_concrete_alt = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. Your task is to generate natural language problem descriptions that involve moving objects between rooms and placing them on specific assets. \\\n",
    "\n",
    "Instructions: \\\n",
    "1. Based on the given 3D scene graph, generate 50 distinct task descriptions that involve: \\\n",
    "   - Picking up an object from one room. \\\n",
    "   - Carrying it to another room. \\\n",
    "   - Placing it on an asset (e.g., table, fridge, chair, bed). \\\n",
    "\n",
    "2. Keep the language natural and friendly, while ensuring clarity by explicitly specifying: \\\n",
    "   - Room identifiers and classes. \\\n",
    "   - Object identifiers and classes. \\\n",
    "   - Asset identifiers and classes. \\\n",
    "\n",
    "3. For each task description, generate a verifying step that ensures the object has been placed on the correct asset. Use the following structured format: \\\n",
    "   - Task Description Example:  \\\n",
    "     \"Please go to bedroom_5, pick up book_9, and place it on dining table_36 in dining room_9.\" \\\n",
    "   - Verifying Step Format:  \\\n",
    "     \"check_object_at(<object_id>, <asset_id>)\" \\\n",
    "   - Verifying Step Example:  \\\n",
    "     \"check_object_at(book_9, table_36)\" \\\n",
    "     \n",
    "Output Format: \\\n",
    "Return the 50 generated tasks in JSON format as follows: \\\n",
    "[ \\\n",
    "  { \"instruction\": \"natural problem description 1\", \"verifying_step\": [\"check_object_at(<object_id>, <asset_id>)\"] }, \\\n",
    "  { \"instruction\": \"natural problem description 2\", \"verifying_step\": [\"check_object_at(<object_id>, <asset_id>)\"] }, \\\n",
    "  ... \\\n",
    "  { \"instruction\": \"natural problem description 50\", \"verifying_step\": \"check_object_at(<object_id>, <asset_id>)\" } \\\n",
    "] \\\n",
    "Ensure that <object_id> and <asset_id> match the corresponding values in each task description. \\\n",
    "### **Rules:** \\\n",
    "1. **Return only valid JSON.** Do **not** use markdown, `json` code blocks (` ```json ... ``` `), or any additional formatting. \\\n",
    "2. **Do not include explanations, extra text, or comments.** Only return the raw JSON. \\\n",
    "3. **Your response must start with `[` and end with `]`, and be valid JSON syntax.** \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#Return **only** valid JSON, with no additional text, explanations, or formatting. The JSON must strictly follow this structure. Avoid ```json suffises and ``` post fixes. \\\n",
    "\n",
    "# 2 asset placement task generation\n",
    "\n",
    "system_prompt_duouble_placement_concrete = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. Your task is to generate natural language problem descriptions that involve moving two objects between rooms and placing them on specific assets. \\\n",
    "\n",
    "Instructions: \\\n",
    "1. Based on the given 3D scene graph, generate 10 distinct task descriptions that involve: \\\n",
    "   - Picking up two different objects from one or more rooms. \\\n",
    "   - Carrying them to another room (or rooms). \\\n",
    "   - Placing each object on a designated asset (e.g., table, fridge, chair, bed). \\\n",
    "\n",
    "2. Keep the language natural and friendly, while ensuring clarity by explicitly specifying: \\\n",
    "   - Room identifiers and classes. \\\n",
    "   - Object identifiers and classes. \\\n",
    "   - Asset identifiers and classes. \\\n",
    "\n",
    "3. For each task description, generate verifying steps that ensure both objects have been placed on the correct assets. Use the following structured format: \\\n",
    "   - Task Description Example:  \\\n",
    "     \"Please go to r_5 bedroom, pick up o_9 book and o_12 cup, and place them on a_36 dining table and a_14 kitchen counter in r_9 dining room.\" \\\n",
    "   - Verifying Step Format:  \\\n",
    "     [\"check_object_at(<object_1_id>, <asset_1_id>)\", \"check_object_at(<object_2_id>, <asset_2_id>)\"] \\\n",
    "\n",
    "Output Format: \\\n",
    "Return the 10 generated tasks in JSON format as follows: \\\n",
    "[ \\\n",
    "  { \"instruction\": \"natural problem description 1\", \"verifying_steps\": [\"check_object_at(<object_1_id>, <asset_1_id>)\", \"check_object_at(<object_2_id>, <asset_2_id>)\"] }, \\\n",
    "  { \"instruction\": \"natural problem description 2\", \"verifying_steps\": [\"check_object_at(<object_1_id>, <asset_1_id>)\", \"check_object_at(<object_2_id>, <asset_2_id>)\"] }, \\\n",
    "  ... \\\n",
    "  { \"instruction\": \"natural problem description 10\", \"verifying_steps\": [\"check_object_at(<object_1_id>, <asset_1_id>)\", \"check_object_at(<object_2_id>, <asset_2_id>)\"] } \\\n",
    "] \\\n",
    "Ensure that <object_1_id>, <object_2_id>, <asset_1_id>, and <asset_2_id> match the corresponding values in each task description. \\\n",
    "Return **only** valid JSON, with no additional text, explanations, or formatting. The JSON must strictly follow this structure. Avoid ```json suffises and ``` post fixes. \\\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# planner prompt\n",
    "\n",
    "planner_prompt_old = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. \\\n",
    "I will provide you with a 3D scene graph in JSON format describing a building and a natural language task. \\\n",
    "Your goal is to generate a step-by-step executable plan to achieve the task using only the allowed actions. \\\n",
    "\n",
    "### **Allowed Actions:** \\\n",
    "You can only use the following predefined actions: \\\n",
    "- `goto(<pose>)`: Move the agent to an adjacent room. Respecting room djacencies is mandatory. Room adjacencies defined under `links` in the JSON scene graph.\\\n",
    "- `access(<asset>)`: Provide access to the set of affordances associated with an asset node and its connected objects. \\\n",
    "- `pickup(<object>)`: Pick up an accessible object from the accessed node. \\\n",
    "- `release(<object>)`: Release a grasped object at an asset node. \\\n",
    "- `turn_on(<object>)` / `turn_off(<object>)`: Toggle an object at the agent’s node, if accessible and has an affordance. \\\n",
    "- `open(<asset>)` / `close(<asset>)`: Open or close an asset at the agent’s node, affecting object accessibility. \\\n",
    "\n",
    "### **Rules and Constraints:** \\\n",
    "1. **Only use objects and assets** that exist in the provided 3D scene graph. \\\n",
    "2. **Ensure room transitions are valid** based on the scene graph connections defined under `links`. Avoid non-adjacent room transitions. The sequence of room transitions should be step-by-step along adjacent rooms in the graph.\\\n",
    "3. **Follow a logical sequence** of actions to complete the task efficiently. \\\n",
    "4. **If an object needs to be picked up**, ensure that the agent has accessed its node before issuing a `pickup` command. \\\n",
    "5. **If an object needs to be placed somewhere**, ensure the correct node is accessed before issuing a `release` command. \\\n",
    "6. **Avoid issuing a `goto` command if the agent is already in the target room. \\\n",
    "7. **Avoid issuing an `access` command to an object before picking up. The `access` command is i=only relevat for assets.\\\n",
    "\n",
    "### **Output Format:** \\\n",
    "Your response must contain **only the list of plan actions** in valid JSON array format, with no additional text, explanation, or formatting. \\\n",
    "Each action must follow this structure: \\\n",
    "[\"goto(r_5)\", \"pickup(o_9)\", \"goto(r_9)\", \"access(a_36)\", \"release(o_9)\", \"close(a_36)\"] \\\n",
    "where r_5, o_9, a_36 are valid nodes from the scene graph. Do not include any extra text, markdown, or explanations. Avoid ```json suffixes and ``` post fixes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "planner_prompt = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. \\\n",
    "I will provide you with a 3D scene graph in JSON format describing a building and a natural language task. \\\n",
    "Your goal is to generate a step-by-step executable plan to achieve the task using only the allowed actions. \\\n",
    "\n",
    "### **Allowed Actions:** \\\n",
    "You can only use the following predefined actions: \\\n",
    "- `goto(<pose>)`: Move the agent **to an adjacent room**. The adjacency is strictly defined under the `links` section of the scene graph. **You must never move between non-adjacent rooms in a single step.** \\\n",
    "- `access(<asset>)`: Provide access to the set of affordances associated with an **asset node** and its connected objects. **Use `access()` only for assets, not objects.** \\\n",
    "- `pickup(<object>)`: Pick up an accessible object **only after the associated asset has been accessed**. \\\n",
    "- `release(<object>)`: Release a grasped object at an asset node. **Ensure the target asset is accessed before releasing.** \\\n",
    "- `turn_on(<object>)` / `turn_off(<object>)`: Toggle an object at the agent’s current node, if accessible and has an affordance. \\\n",
    "- `open(<asset>)` / `close(<asset>)`: Open or close an asset at the agent’s current node, affecting object accessibility. \\\n",
    "\n",
    "### **Rules and Constraints:** \\\n",
    "1. **Room transitions must be strictly adjacent.** The agent can only move along the adjacency links defined in the scene graph. Multi-step movements are not allowed in a single `goto()` command. Instead, find the shortest valid path using adjacency constraints. \\\n",
    "2. **Plan actions in the correct sequence:** Move → Access → Pickup → Move → Release. Avoid skipping or reordering steps. \\\n",
    "3. **Ensure objects are accessible before interacting:** \\\n",
    "   - Use `access(<asset>)` **before** `pickup(<object>)`. \\\n",
    "   - Use `access(<asset>)` **before** `release(<object>)`. \\\n",
    "   - Never attempt `pickup()` or `release()` without ensuring accessibility first. \\\n",
    "4. **Avoid redundant actions:** \\\n",
    "   - Do not issue `goto(<pose>)` if the agent is already in the target location. \\\n",
    "   - Do not issue `access(<asset>)` if the asset is already accessed. \\\n",
    "5. **If an invalid action is detected, replan accordingly:** \\\n",
    "   - If an invalid `goto(<pose>)` action occurs, verify the adjacency and correct the path. \\\n",
    "   - If an invalid `pickup(<object>)` occurs, check if `access(<asset>)` is missing and fix it. \\\n",
    "   - If an invalid `release(<object>)` occurs, ensure the correct asset is accessed before releasing. \\\n",
    "\n",
    "### **Output Format:** \\\n",
    "Your response must contain **only the list of plan actions** in valid JSON array format, with no additional text, explanation, or formatting. \\\n",
    "Each action must follow this structure: \\\n",
    "[\"goto(r_5)\", \"pickup(o_9)\", \"goto(r_9)\", \"access(a_36)\", \"release(o_9)\", \"close(a_36)\"] \\\n",
    "where `r_5`, `o_9`, and `a_36` are valid nodes from the scene graph. **Do not include any extra text, markdown, or explanations.** Avoid `json` suffixes and ` ``` ` post fixes. \\\n",
    "\"\"\"\n",
    "\n",
    "planner_prompt_alt = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. \\\n",
    "I will provide you with a 3D scene graph in JSON format describing a building and a natural language task. \\\n",
    "Your goal is to generate a step-by-step executable plan to achieve the task using only the allowed actions. \\\n",
    "\n",
    "### **Allowed Actions:** \\\n",
    "You can only use the following predefined actions: \\\n",
    "- `goto(<pose>)`: Move the agent **to an adjacent room**. The adjacency is strictly defined under the `links` section of the scene graph. **You must never move between non-adjacent rooms in a single step.** \\\n",
    "- `access(<asset>)`: Provide access to the set of affordances associated with an **asset node** and its connected objects. **Use `access()` only for assets, not objects.** \\\n",
    "- `pickup(<object>)`: Pick up an accessible object **only after the associated asset has been accessed**. \\\n",
    "- `release(<object>)`: Release a grasped object at an asset node. **Ensure the target asset is accessed before releasing.** \\\n",
    "- `turn_on(<object>)` / `turn_off(<object>)`: Toggle an object at the agent’s current node, if accessible and has an affordance. \\\n",
    "- `open(<asset>)` / `close(<asset>)`: Open or close an asset at the agent’s current node, affecting object accessibility. \\\n",
    "\n",
    "### **Rules and Constraints:** \\\n",
    "1. **Room transitions must be strictly adjacent.** The agent can only move along the adjacency links defined in the scene graph. Multi-step movements are not allowed in a single `goto()` command. Instead, find the shortest valid path using adjacency constraints. \\\n",
    "2. **Plan actions in the correct sequence:** Move → Access → Pickup → Move → Release. Avoid skipping or reordering steps. \\\n",
    "3. **Ensure objects are accessible before interacting:** \\\n",
    "   - Use `access(<asset>)` **before** `pickup(<object>)`. \\\n",
    "   - Use `access(<asset>)` **before** `release(<object>)`. \\\n",
    "   - Never attempt `pickup()` or `release()` without ensuring accessibility first. \\\n",
    "4. **Avoid redundant actions:** \\\n",
    "   - Do not issue `goto(<pose>)` if the agent is already in the target location. \\\n",
    "   - Do not issue `access(<asset>)` if the asset is already accessed. \\\n",
    "5. **If an invalid action is detected, replan accordingly:** \\\n",
    "   - If an invalid `goto(<pose>)` action occurs, verify the adjacency and correct the path. \\\n",
    "   - If an invalid `pickup(<object>)` occurs, check if `access(<asset>)` is missing and fix it. \\\n",
    "   - If an invalid `release(<object>)` occurs, ensure the correct asset is accessed before releasing. \\\n",
    "\n",
    "### **Output Format:** \\\n",
    "Your response must contain **only the list of plan actions** in valid JSON array format, with no additional text, explanation, or formatting. \\\n",
    "Each action must follow a structure like this: \\\n",
    "[\"action_1(<parameter>)\", \"action_2(<parameter>)\", \"action_3(<parameter>)\", \"action_4(<parameter>)\", \"action_5(<parameter>)\", \"action_6(<parameter>)\"] \\\n",
    "Wher (<parameter>) are valid nodes from the scene graph, like rooms, objects or assets.\n",
    "An example of a plan with a series of actions using a passive asset:\n",
    "[\"goto(bedroom_5)\", \"pickup(book_9)\", \"goto(corridor_7)\", \"goto(kitchen_9)\", \"access(dining table_36)\", \"release(book_9)\"] \\\n",
    "An example of a plan with a series of actions using a openable asset: \\\n",
    "[\"goto(bedroom_5)\", \"pickup(bottle_2)\", \"goto(corridor_7)\", \"goto(kitchen_9)\", \"access(refridgerator_10)\", \"open(refridgerator_10)\", \"release(bottle_2)\", \"close(refridgerator_10)\"] \\\n",
    "**Do not include any extra text, markdown, or explanations.** Avoid `json` suffixes and ` ``` ` post fixes. \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# planner prompt\n",
    "\n",
    "graph_pruning_prompt = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. \\\n",
    "I will provide you with a 3D scene graph in JSON format describing a building and a natural language task. \\\n",
    "Your goal is to identify the **relevant rooms** involved in the task, specifically: \\\n",
    "- The room where an object needs to be picked up. \\\n",
    "- The room where an object needs to be placed. \\\n",
    "- Any additional room required to access an asset. \\\n",
    "\n",
    "### **Output Format:** \\\n",
    "Return the list of relevant room IDs **only** in JSON array format. \\\n",
    "Each room should appear **only once**, even if multiple actions occur in the same room. \\\n",
    "Do not include any extra text, explanations, or formatting. Avoid `json` prefixes or suffixes. \\\n",
    "\n",
    "### **Example:** \\\n",
    "#### **Input Task:**  \n",
    "*\"Please go to r_5 (bedroom), pick up o_9 (book), and place it on a_36 (dining table) in r_9 (dining room).\"*  \n",
    "\n",
    "#### **Expected Output:**  \n",
    "[\"r_5\", \"r_9\"]\n",
    "Avoid ```json suffixes and ``` post fixes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "graph_pruning_prompt_alt = \"\"\"\\\n",
    "You are an expert in robotic 3D scene graph planning. \\\n",
    "I will provide you with a 3D scene graph in JSON format describing a building and a natural language task. \\\n",
    "Your goal is to identify the **relevant rooms** involved in the task, specifically: \\\n",
    "- The room where an object needs to be picked up. \\\n",
    "- The room where an object needs to be placed. \\\n",
    "- Any additional room required to access an asset. \\\n",
    "\n",
    "### **Output Format:** \\\n",
    "Return the list of relevant room IDs **only** in JSON array format. \\\n",
    "Each room should appear **only once**, even if multiple actions occur in the same room. \\\n",
    "Do not include any extra text, explanations, or formatting. Avoid `json` prefixes or suffixes. \\\n",
    "\n",
    "### **Example:** \\\n",
    "#### **Input Task:**  \n",
    "*\"Please go to bedroom_5, pick up book_9, and place it on dining table_36 in dining room_9.\"*  \n",
    "\n",
    "#### **Expected Output:**  \n",
    "[\"bedroom_5\", \"dining room_9\"]\n",
    "Avoid ```json suffixes and ``` post fixes.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def clean_json_response(response_text):\n",
    "    \"\"\"Removes ```json and ``` formatting from the response.\"\"\"\n",
    "    return re.sub(r\"```json\\s*|\\s*```\", \"\", response_text).strip()\n",
    "\n",
    "# task and validator generator\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "\n",
    "instruction_gen_message = [\n",
    "    (\n",
    "        \"system\",\n",
    "        system_prompt_single_placement_concrete_alt,\n",
    "    ),\n",
    "    \n",
    "    (\"human\", \"The 3D scene graph in JSON format:\"),\n",
    "    (\"human\", json_graph),\n",
    "]\n",
    "\n",
    "token_count = count_tokens(instruction_gen_message, model=\"gpt-4\")\n",
    "print(f\"🔢 Total Tokens: {token_count}\")\n",
    "\n",
    "instruction_reply = llm_gemini.invoke(instruction_gen_message)\n",
    "generated_instructions = instruction_reply.content\n",
    "\n",
    "generated_instructions = clean_json_response(generated_instructions)\n",
    "\n",
    "\n",
    "#print(generated_instructions)\n",
    "\n",
    "instructions = json.loads(generated_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generated_instructions)\n",
    "\n",
    "print(len(instructions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() \n",
    "\n",
    "# Allow asyncio to run inside Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def invoke_with_timeout(llm, message, timeout=10):\n",
    "    \"\"\"Runs LLM invocation asynchronously with a strict timeout.\"\"\"\n",
    "    try:\n",
    "        return await asyncio.wait_for(llm.agenerate(messages=[message]), timeout=timeout)\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"⏳ LLM invocation timed out. Skipping...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Specify the directory path\n",
    "scenegraph_directory = Path('/home/laszlo/Stanford/3dscenegraph/medium/automated_graph/')\n",
    "\n",
    "# Get all .npz files in the directory\n",
    "npz_files = list(scenegraph_directory.glob(\"*.npz\"))\n",
    "\n",
    "# Convert to string paths if needed\n",
    "npz_file_paths = [str(f) for f in npz_files]\n",
    "\n",
    "print(npz_file_paths)\n",
    "print(len(npz_file_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAGEN LOOP\n",
    "\n",
    "# addding tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for scenegraph_path in tqdm(npz_file_paths, desc=\"Generating tasks...\", total=len(npz_file_paths)):\n",
    "\n",
    "    building = loader(scenegraph_path)\n",
    "    scenegraph_original = LSSceneGraph(building)\n",
    "    scenegraph = copy.deepcopy(scenegraph_original)\n",
    "\n",
    "    print(\"Scenegraph loaded. Generating instructions...\")\n",
    "\n",
    "\n",
    "    scenegraph = copy.deepcopy(scenegraph_original)\n",
    "    json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "\n",
    "    instruction_gen_message = [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt_single_placement_concrete_alt,\n",
    "        ),\n",
    "        \n",
    "        (\"human\", \"The 3D scene graph in JSON format:\"),\n",
    "        (\"human\", json_graph),\n",
    "    ]\n",
    "\n",
    "    token_count = count_tokens(instruction_gen_message, model=\"gpt-4\")\n",
    "    print(f\"Total Tokens: {token_count}\")\n",
    "\n",
    "    instruction_reply = llm_gemini.invoke(instruction_gen_message)\n",
    "    generated_instructions = instruction_reply.content\n",
    "    generated_instructions = clean_json_response(generated_instructions)\n",
    "\n",
    "    instructions = json.loads(generated_instructions)\n",
    "\n",
    "    print(\"Generated instructions: \", len(instructions))\n",
    "\n",
    "    print(\"Generating plans...\")\n",
    "\n",
    "    exec_time  = time.time()\n",
    "\n",
    "    training_data = {\"valid\": [], \"invalid\": [], \"stats\": {}}\n",
    "\n",
    "    for i in range(len(instructions)):\n",
    "\n",
    "        task_description = instructions[i][\"instruction\"]\n",
    "\n",
    "        #print(task_description)\n",
    "\n",
    "        scenegraph = copy.deepcopy(scenegraph_original)\n",
    "        json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "\n",
    "        #pruned_json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt)\n",
    "\n",
    "        try:\n",
    "            pruned_json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error pruning scenegraph: {e}. Skipping pruning.\")\n",
    "            pruned_json_graph = json_graph  # Use original graph if pruning fails\n",
    "\n",
    "        #plot_scene_graph(scenegraph.get_scene_graph_subset())\n",
    "\n",
    "        room_graph = json.dumps(scenegraph.get_room_graph(), indent=4)\n",
    "        \n",
    "        plan_gen_message = [\n",
    "            SystemMessage(content=planner_prompt_alt),\n",
    "            HumanMessage(content=\"The room adjacencies to strictly follow:\"),\n",
    "            HumanMessage(content=room_graph),\n",
    "            HumanMessage(content=\"The 3D scene graph in JSON format:\"),\n",
    "            HumanMessage(content=json_graph),\n",
    "            HumanMessage(content=\"The task description:\"),\n",
    "            HumanMessage(content=task_description)\n",
    "        ]\n",
    "\n",
    "        #print(plan_gen_message)\n",
    "\n",
    "        plan_gen_llm_reply = await invoke_with_timeout(llm_gemini, plan_gen_message, timeout=15)\n",
    "\n",
    "        if plan_gen_llm_reply is not None:\n",
    "\n",
    "            generated_plan = plan_gen_llm_reply.generations[0][0].text\n",
    "\n",
    "            generated_plan = clean_json_response(generated_plan)\n",
    "\n",
    "            #plan = json.loads(generated_plan)\n",
    "\n",
    "            try:\n",
    "                plan = json.loads(generated_plan)\n",
    "            except Exception as e:\n",
    "                print(f\" Error pruning plan: {e}. Skipping plan.\")\n",
    "                continue\n",
    "\n",
    "            #print(plan)\n",
    "\n",
    "            exec_result = scenegraph.execute_plan(plan)\n",
    "            verifying_step = instructions[i][\"verifying_step\"]\n",
    "            validate_result = scenegraph.check_objects_classes(verifying_step)\n",
    "            \n",
    "            print(f\"[Plan {i}]\")\n",
    "            print(\"  >\", exec_result)\n",
    "            print(\"  >\", validate_result)\n",
    "\n",
    "            if exec_result == \"Valid plan.\" and validate_result == \"Valid plan.\":\n",
    "                print(\"  Saving valid plan.\")\n",
    "\n",
    "                scene_graph = json_graph\n",
    "                prunded_scene_graph = pruned_json_graph\n",
    "                system_prompt = planner_prompt_alt\n",
    "                room_graph = room_graph\n",
    "                task_description = task_description\n",
    "                verifying_step = json.dumps(verifying_step)\n",
    "                plan = plan\n",
    "\n",
    "                data_node = {\n",
    "                    \"scene_graph\" : scene_graph,\n",
    "                    \"pruned_graph\" : prunded_scene_graph,\n",
    "                    \"system_prompt\" : system_prompt,\n",
    "                    \"room_graph\" : room_graph,\n",
    "                    \"instruction\" : task_description,\n",
    "                    \"validation_steps\" : verifying_step,\n",
    "                    \"plan\" : plan,\n",
    "                }\n",
    "\n",
    "                training_data[\"valid\"].append(data_node)\n",
    "\n",
    "            else:\n",
    "                print(\"  Saving invalid plan.\")\n",
    "\n",
    "                prunded_scene_graph = pruned_json_graph\n",
    "                system_prompt = planner_prompt_alt\n",
    "                room_graph = room_graph\n",
    "                task_description = task_description\n",
    "                verifying_step = json.dumps(verifying_step)\n",
    "                plan = plan\n",
    "\n",
    "                data_node = {\n",
    "                    \"scene_graph\" : scene_graph,\n",
    "                    \"pruned_graph\" : prunded_scene_graph,\n",
    "                    \"system_prompt\" : system_prompt,\n",
    "                    \"room_graph\" : room_graph,\n",
    "                    \"instruction\" : task_description,\n",
    "                    \"validation_steps\" : verifying_step,\n",
    "                    \"plan\" : plan,\n",
    "                }\n",
    "\n",
    "                training_data[\"invalid\"].append(data_node)\n",
    "\n",
    "    exec_time = time.time() - exec_time\n",
    "\n",
    "    print(\"Valid Plans: \", len(training_data[\"valid\"]))\n",
    "    print(\"Invalid Plans: \", len(training_data[\"invalid\"]))\n",
    "    print(\"Total Plans: \", len(instructions))\n",
    "\n",
    "    training_data[\"stats\"].update({\n",
    "        \"valid_plans\": len(training_data[\"valid\"]),\n",
    "        \"invalid_plans\": len(training_data[\"invalid\"]),\n",
    "        \"total_plans\": len(instructions),\n",
    "        \"exec_time\": exec_time,\n",
    "    })\n",
    "\n",
    "    print(\"Saving training data...\")\n",
    "    scenegraph_file = Path(scenegraph_path)\n",
    "    data_path = scenegraph_file.with_name(scenegraph_file.stem + \"_single.json\")\n",
    "\n",
    "    with open(data_path, \"w\") as f:\n",
    "        json.dump(training_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = {\"valid\": [], \"invalid\": []}\n",
    "\n",
    "\n",
    "scene_graph = \n",
    "prunded_scene_graph = \n",
    "system_prompt = \n",
    "room_graph = \n",
    "task_description = \n",
    "verifying_step =\n",
    "plan = \n",
    "\n",
    "data_node = {\n",
    "    \"scene_graph\" : scene_graph,\n",
    "    \"pruned_graph\" : prunded_scene_graph,\n",
    "    \"system_prompt\" : system_prompt,\n",
    "    \"room_graph\" : room_graph,\n",
    "    \"instruction\" : task_description,\n",
    "    \"validation_steps\" : verifying_step,\n",
    "    \"plan\" : plan,\n",
    "}\n",
    "\n",
    "training_data[\"valid\"].append(data_node)\n",
    "\n",
    "\n",
    "\n",
    "data_node = {\n",
    "    \"scene_graph\" : scene_graph,\n",
    "    \"pruned_graph\" : prunded_scene_graph,\n",
    "    \"system_prompt\" : system_prompt,\n",
    "    \"room_graph\" : room_graph,\n",
    "    \"instruction\" : task_description,\n",
    "    \"validation_steps\" : verifying_step,\n",
    "    \"plan\" : plan,\n",
    "}\n",
    "\n",
    "training_data[\"invalid\"].append(data_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save training data to file\n",
    "\n",
    "\n",
    "scenegraph_file = Path(scenegraph_path)\n",
    "data_path = scenegraph_file.with_name(scenegraph_file.stem + \"_single.json\")\n",
    "\n",
    "with open(data_path, \"w\") as f:\n",
    "    json.dump(training_data, f, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenegraph_file = Path(scenegraph_path)\n",
    "json_path = scenegraph_file.with_name(scenegraph_file.stem + \"_single.json\")\n",
    "\n",
    "print(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plot_scene_graph(scenegraph.get_scene_graph())\n",
    "json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "print(json_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenegraph_path = '/home/laszlo/Stanford/3dscenegraph/tiny/verified_graph/3DSceneGraph_Benevolence.npz'\n",
    "building = loader(scenegraph_path)\n",
    "scenegraph_original = LSSceneGraph(building)\n",
    "\n",
    "plot_scene_graph(scenegraph.get_scene_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plan tester, no loop\n",
    "\n",
    "i = 0\n",
    "\n",
    "task_description = instructions[i][\"instruction\"]\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "#json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "\n",
    "json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt)\n",
    "room_graph = json.dumps(scenegraph.get_room_graph(), indent=4)\n",
    "\n",
    "print(task_description)\n",
    "\n",
    "#plot_scene_graph_json(json_graph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plan_gen_message = [\n",
    "    (\n",
    "        \"system\",\n",
    "        planner_prompt_alt,\n",
    "    ),\n",
    "    (\"system\", \"The room adjacencies to strictly follow:\"),\n",
    "    (\"system\", room_graph),\n",
    "    (\"human\", \"The 3D scene graph in JSON format:\"),\n",
    "    (\"human\", json_graph),\n",
    "    (\"human\", \"The task description:\"),\n",
    "    (\"human\", task_description),\n",
    "]\n",
    "\n",
    "#token_count = count_tokens(instruction_gen_message, model=\"gpt-4\")\n",
    "#print(f\"Total Tokens: {token_count}\")\n",
    "\n",
    "plan_gen_llm_reply = llm_gemini.invoke(plan_gen_message)\n",
    "generated_plan = plan_gen_llm_reply.content\n",
    "\n",
    "generated_plan = clean_json_response(generated_plan)\n",
    "\n",
    "\n",
    "print(generated_plan)\n",
    "\n",
    "\n",
    "plan = json.loads(generated_plan)\n",
    "\n",
    "result = scenegraph.execute_plan(plan)\n",
    "print(f\"Plan {i}: {result}\")\n",
    "\n",
    "verifying_step = instructions[i][\"verifying_step\"]\n",
    "\n",
    "print(verifying_step)\n",
    "\n",
    "\n",
    "result = scenegraph.check_objects_classes(verifying_step)\n",
    "print(f\"Plan {i}: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_scene_graph(scenegraph.get_scene_graph())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/SFT/llama3_1_8B/lora/epoch_4\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model and force to GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map={\"\": 0}  # Force GPU\n",
    ")\n",
    "\n",
    "# Confirm model is on CUDA\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Prepare input\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Ensure inputs are on CUDA\n",
    "print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plan tester, no loop LLAMA\n",
    "\n",
    "i = 0\n",
    "\n",
    "task_description = instructions[i][\"instruction\"]\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "#json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "\n",
    "json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt)\n",
    "room_graph = json.dumps(scenegraph.get_room_graph(), indent=4)\n",
    "\n",
    "print(task_description)\n",
    "\n",
    "#plot_scene_graph_json(json_graph)\n",
    "\n",
    "#plan_gen_message = [\n",
    "#    (\"system\", planner_prompt_alt),\n",
    "#    (\"human\", f\"The room adjacencies to strictly follow: {room_graph} The 3D Scene Graph in JSON format: {json_graph} The task description: {task_description}\"),\n",
    "#]\n",
    "\n",
    "plan_gen_message = f\"System: {planner_prompt_alt}\\nHuman: The room adjacencies to strictly follow: {room_graph} The 3D Scene Graph in JSON format: {json_graph} The task description: {task_description}\"\n",
    "\n",
    "#token_count = count_tokens(instruction_gen_message, model=\"gpt-4\")\n",
    "#print(f\"Total Tokens: {token_count}\")\n",
    "\n",
    "#plan_gen_llm_reply = llm_llama.invoke(plan_gen_message)\n",
    "\n",
    "#plan_gen_message = \"Hello, how are you?\"\n",
    "\n",
    "inputs = tokenizer(plan_gen_message, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"-------------------\")\n",
    "#print(plan_gen_llm_reply)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/SFT/llama3_1_8B/lora/epoch_12\"\n",
    "#model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map={\"\": 0}  # Force GPU\n",
    ")\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "#pipe = pipeline(\n",
    "#    \"text-generation\",\n",
    "#    model=model,\n",
    "#    tokenizer=tokenizer,\n",
    "#    do_sample=False,  # Deterministic output\n",
    "#    return_full_text=False,\n",
    "#    max_new_tokens=10000\n",
    "#)\n",
    "\n",
    "#pipe = pipeline(\n",
    "#    \"text-generation\",\n",
    "#    model=model,\n",
    "#    tokenizer=tokenizer,\n",
    "#    do_sample=True,  \n",
    "#    temperature=0.3,  # Lower temperature for more deterministic output\n",
    "#    top_p=0.9,        # Control diversity\n",
    "#    return_full_text=False,\n",
    "#    min_new_tokens=10,  # Ensure at least some output\n",
    "#    max_new_tokens=300  # Adjust based on expected plan length\n",
    "#    eos_token_id=tokenizer.convert_tokens_to_ids(\"]\")\n",
    "#)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=False,  # Disable sampling for deterministic planning\n",
    "    num_beams=5,      # Increase beam width for better search\n",
    "    return_full_text=False,\n",
    "    min_new_tokens=10,\n",
    "    max_new_tokens=500,\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(\"]\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm_llama = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "response = llm_llama.invoke(\"Hello, how are you?\")\n",
    "\n",
    "print(\"Generated response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "\n",
    "task_description = instructions[i][\"instruction\"]\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "#json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "\n",
    "json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt)\n",
    "room_graph = json.dumps(scenegraph.get_room_graph(), indent=4)\n",
    "\n",
    "print(task_description)\n",
    "\n",
    "#plot_scene_graph_json(json_graph)\n",
    "\n",
    "#plan_gen_message = [\n",
    "#    (\"system\", planner_prompt_alt),\n",
    "#    (\"human\", f\"The room adjacencies to strictly follow: {room_graph} The 3D Scene Graph in JSON format: {json_graph} The task description: {task_description}\"),\n",
    "#]\n",
    "\n",
    "plan_gen_message = f\"System: {planner_prompt_alt}\\nHuman: The room adjacencies to strictly follow: {room_graph} The 3D Scene Graph in JSON format: {json_graph} The task description: {task_description}\"\n",
    "\n",
    "#print(plan_gen_message)\n",
    "\n",
    "#token_count = count_tokens(instruction_gen_message, model=\"gpt-4\")\n",
    "\n",
    "\n",
    "#plan_gen_message = \"Hello, how are you?\"\n",
    "\n",
    "plan_gen_llm_reply = llm_llama.invoke(plan_gen_message)\n",
    "\n",
    "\n",
    "\n",
    "print(plan_gen_llm_reply)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_scene_graph_json(json_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = pipe(\"Hello, how are you?\")\n",
    "print(\"Pipeline test output:\", test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifying_step = instructions[i][\"verifying_step\"]\n",
    "\n",
    "print(verifying_step)\n",
    "\n",
    "\n",
    "result = scenegraph.check_objects_classes(verifying_step)\n",
    "print(f\"Plan {i}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "\n",
    "task_description = instructions[i][\"instruction\"]\n",
    "\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "\n",
    "json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt_alt)\n",
    "print(json_graph)\n",
    "\n",
    "print(task_description)\n",
    "\n",
    "plot_scene_graph_json(json_graph)\n",
    "\n",
    "\n",
    "#room_graph = json.dumps(scenegraph.get_room_graph(), indent=4)\n",
    "\n",
    "#print(room_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 9\n",
    "\n",
    "task_description = instructions[i][\"instruction\"]\n",
    "instruction = instructions[i]\n",
    "\n",
    "scenegraph = copy.deepcopy(scenegraph_original)\n",
    "\n",
    "json_graph = json.dumps(scenegraph.get_scene_graph(), indent=4)\n",
    "#json_graph = prune_scenegraph(scenegraph, task_description, llm_gemini, graph_pruning_prompt)\n",
    "room_graph = json.dumps(scenegraph.get_room_graph(), indent=4)\n",
    "\n",
    "\n",
    "#results = generate_plan_feecback(task_description, scenegraph, json_graph, llm_gemini, planner_prompt, max_retries=10)\n",
    "#results = generate_plan_feedback(instruction, scenegraph, json_graph, room_graph, llm_gemini, planner_prompt, max_retries=10)\n",
    "\n",
    "\n",
    "#results = generate_plan_feedback_gemini_timeout(instruction, scenegraph, json_graph, room_graph, llm_gemini, planner_prompt, max_retries=10)\n",
    "\n",
    "results = await generate_plan_feedback_gemini_timeout(instruction, scenegraph, json_graph, room_graph, llm_gemini, planner_prompt_alt, max_retries=3)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
