{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training with torchtune notes...\n",
    "\n",
    "import torch\n",
    "from torchtune import TuneConfig\n",
    "from torchtune.models.llama import create_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the base Llama model in Torchtune\n",
    "model = create_model(model_name, quantization=\"bnb_4bit\")  # Load model with 4-bit quantization\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from JSONL file\n",
    "dataset = load_dataset(\"json\", data_files=\"fine_tuning_data.jsonl\", split=\"train\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.rename_column(\"expected\", \"labels\")  # Rename correct output to labels\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "from torchtune.trainers import Trainer\n",
    "\n",
    "# Define fine-tuning config\n",
    "train_config = TuneConfig(\n",
    "    output_dir=\"./fine_tuned_llama\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    config=train_config,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_llama\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "fine_tuned_model_name = \"./fine_tuned_llama\"\n",
    "\n",
    "# Load fine-tuned model for inference\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model_name, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Integrate with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Test inference with fine-tuned model\n",
    "query = \"What is the capital of France?\"\n",
    "response = llm.invoke(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SFT data format: ChatML or Alpaca-style data format, save in .jsonl file\n",
    "\n",
    "{\"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a robotic 3D scene graph planning expert. Generate a valid plan using the given JSON scene graph.\"},\n",
    "    {\"role\": \"user\", \"content\": \"3D Scene Graph: {scene_graph_here} Task: {task_description}\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"[\\\"goto(r_10)\\\", \\\"goto(r_6)\\\", \\\"pickup(o_9)\\\", \\\"goto(r_9)\\\", \\\"release(o_9)\\\"]\"}\n",
    "]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reference message:\n",
    "\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"{planner_prompt_alt}\"},\n",
    "        {\"role\": \"user\", \"content\": \"The room adjacencies to strictly follow:\"},\n",
    "        {\"role\": \"user\", \"content\": \"{room_graph}\"},\n",
    "        {\"role\": \"user\", \"content\": \"The 3D Scene Graph in JSON format:\"},\n",
    "        {\"role\": \"user\", \"content\": \"{json_graph}\"},\n",
    "        {\"role\": \"user\", \"content\": \"The task description:\"},\n",
    "        {\"role\": \"user\", \"content\": \"{task_description}\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"{plan}\"}\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "directory = '/home/laszlo/Stanford/3dscenegraph/tiny/verified_graph'\n",
    "json_file = '3DSceneGraph_Allensville_single.json'\n",
    "\n",
    "file_path = os.path.join(directory, json_file)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "print(len(data[\"valid\"]))\n",
    "\n",
    "valid_data = data[\"valid\"][0]\n",
    "\n",
    "scene_graph = valid_data[\"scene_graph\"]\n",
    "prunded_scene_graph = valid_data[\"pruned_graph\"]\n",
    "system_prompt = valid_data[\"system_prompt\"]\n",
    "room_graph = valid_data[\"room_graph\"]\n",
    "task_description = valid_data[\"instruction\"]\n",
    "verifying_step = valid_data[\"validation_steps\"]\n",
    "plan = valid_data[\"plan\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(scene_graph)\n",
    "#print(prunded_scene_graph)\n",
    "#print(system_prompt)\n",
    "#print(room_graph)\n",
    "#print(task_description)\n",
    "#print(verifying_step)\n",
    "print(plan)\n",
    "\n",
    "# converting list to json string\n",
    "plan_json = json.dumps(plan)\n",
    "print(plan_json.type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT data loader\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sft_training_data_loader(directory):\n",
    "    \n",
    "    examples = []\n",
    "\n",
    "    # Get list of JSON files in the directory\n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "\n",
    "    print(f\"üîç Found {len(json_files)} JSON files in {directory}\")\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {directory}\")\n",
    "        return\n",
    "\n",
    "    # Iterate over JSON files and yield parsed content\n",
    "    for json_file in tqdm(json_files , desc=\"Processing JSON files\", total=len(json_files)):\n",
    "        file_path = os.path.join(directory, json_file)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for valid_data in data[\"valid\"]:\n",
    "\n",
    "            scene_graph = valid_data[\"scene_graph\"]\n",
    "            prunded_scene_graph = valid_data[\"pruned_graph\"]\n",
    "            system_prompt = valid_data[\"system_prompt\"]\n",
    "            room_graph = valid_data[\"room_graph\"]\n",
    "            task_description = valid_data[\"instruction\"]\n",
    "            verifying_step = valid_data[\"validation_steps\"]\n",
    "            plan = json.dumps(valid_data[\"plan\"])\n",
    "\n",
    "            examples.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"The room adjacencies to strictly follow: {room_graph} The 3D Scene Graph in JSON format: {prunded_scene_graph} The task description: {task_description}\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{plan}\"}\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    # üöÄ Write the entire list to JSONL format\n",
    "\n",
    "    save_path = os.path.join(directory, \"sft_trainer_dataset.jsonl\")\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in examples:\n",
    "            f.write(json.dumps(example) + \"\\n\")  # Write each JSON object on a new line\n",
    "\n",
    "    print(\"JSONL file saved successfully!\")\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#datagen_output_directory = '/home/laszlo/Stanford/3dscenegraph/datagen_output_tiny'\n",
    "datagen_output_directory = '/home/laszlo/Stanford/3dscenegraph/datagen_output_merged'\n",
    "\n",
    "\n",
    "training_examples = sft_training_data_loader(datagen_output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stats data loader\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def stats_data_loader(directory):\n",
    "    \n",
    "    examples = []\n",
    "\n",
    "    # Get list of JSON files in the directory\n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "\n",
    "    print(f\"üîç Found {len(json_files)} JSON files in {directory}\")\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {directory}\")\n",
    "        return\n",
    "\n",
    "    valid_plans = 0\n",
    "    invalid_plans = 0\n",
    "    total_plans = 0\n",
    "    exec_time = 0\n",
    "    length = 0\n",
    "\n",
    "    # Iterate over JSON files and yield parsed content\n",
    "    for json_file in tqdm(json_files , desc=\"Processing JSON files\", total=len(json_files)):\n",
    "        file_path = os.path.join(directory, json_file)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        valid_plans += data[\"stats\"][\"valid_plans\"]\n",
    "        invalid_plans += data[\"stats\"][\"invalid_plans\"]\n",
    "        total_plans += data[\"stats\"][\"total_plans\"]\n",
    "        exec_time += data[\"stats\"][\"exec_time\"]\n",
    "\n",
    "\n",
    "        for valid_data in data[\"valid\"]:\n",
    "\n",
    "            length += len(valid_data[\"plan\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Valid plans: {valid_plans}\")\n",
    "    print(f\"Invalid plans: {invalid_plans}\")\n",
    "    print(f\"Total plans: {total_plans}\")\n",
    "    print(f\"Execution time sum: {exec_time}\")\n",
    "    print(f\"Average execution time: {exec_time/(valid_plans + invalid_plans)}\")\n",
    "    print(f\"Average plan length: {length/valid_plans}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#datagen_output_directory = '/home/laszlo/Stanford/3dscenegraph/datagen_output_tiny'\n",
    "datagen_output_directory = '/home/laszlo/Stanford/3dscenegraph/datagen_output_merged'\n",
    "\n",
    "\n",
    "training_examples = stats_data_loader(datagen_output_directory)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO data loader\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ppo_training_data_loader(directory):\n",
    "    \n",
    "    examples = []\n",
    "\n",
    "    # Get list of JSON files in the directory\n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "\n",
    "    print(f\"üîç Found {len(json_files)} JSON files in {directory}\")\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\" No JSON files found in {directory}\")\n",
    "        return\n",
    "\n",
    "    # Iterate over JSON files and yield parsed content\n",
    "    for json_file in tqdm(json_files , desc=\"Processing JSON files\", total=len(json_files)):\n",
    "        file_path = os.path.join(directory, json_file)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for valid_data in data[\"valid\"]:\n",
    "\n",
    "            scene_graph = valid_data[\"scene_graph\"]\n",
    "            prunded_scene_graph = valid_data[\"pruned_graph\"]\n",
    "            system_prompt = valid_data[\"system_prompt\"]\n",
    "            room_graph = valid_data[\"room_graph\"]\n",
    "            task_description = valid_data[\"instruction\"]\n",
    "            verifying_step = valid_data[\"validation_steps\"]\n",
    "            plan = json.dumps(valid_data[\"plan\"])\n",
    "\n",
    "            examples.append({\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"room_graph\": room_graph,\n",
    "                \"scene_graph\": prunded_scene_graph,\n",
    "                \"task_description\": task_description,\n",
    "                \"verifying_step\": verifying_step,\n",
    "                \"plan\": plan\n",
    "            })\n",
    "\n",
    "    # üöÄ Write the entire list to JSONL format\n",
    "\n",
    "    save_path = os.path.join(directory, \"ppo_trainer_dataset.jsonl\")\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in examples:\n",
    "            f.write(json.dumps(example) + \"\\n\")  # Write each JSON object on a new line\n",
    "\n",
    "    print(\"JSONL file saved successfully!\")\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_output_directory = '/home/laszlo/Stanford/3dscenegraph/datagen_output_merged'\n",
    "\n",
    "\n",
    "training_examples = ppo_training_data_loader(datagen_output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toke counter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# **1Ô∏èLoad Model & Tokenizer**\n",
    "model_name = \"/home/laszlo/Stanford/LangSpace/meta-llama/Llama-3.2-3B\"  # Change this to 1B, 3B, or 8B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_example_token_count(example):\n",
    "    \"\"\"\n",
    "    Tokenizes a dataset example and returns the token count.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    for message in example[\"messages\"]:\n",
    "        full_text += message[\"content\"] + \" \"  # Concatenate all message content\n",
    "\n",
    "    tokens = tokenizer(full_text, truncation=False, return_tensors=\"pt\")  # Tokenize without truncation\n",
    "    return tokens[\"input_ids\"].shape[1]  # Token count\n",
    "\n",
    "# Example usage on the dataset\n",
    "token_counts = [get_example_token_count(example) for example in training_examples]\n",
    "\n",
    "# Print statistics\n",
    "max_length = max(token_counts)\n",
    "average_length = sum(token_counts) / len(token_counts)\n",
    "\n",
    "print(f\"Max Token Length: {max_length}\")\n",
    "print(f\"Average Token Length: {average_length:.2f}\")\n",
    "\n",
    "# Adjust `max_seq_length` accordingly\n",
    "recommended_max_seq_length = min(max_length, 1024)  # Keep within model limits\n",
    "print(f\"Recommended max_seq_length: {recommended_max_seq_length}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torchtune SFT install\n",
    "# pip install torchtune torch torchvision torchaudio transformers accelerate bitsandbytes peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRL PPO data format:\n",
    "\n",
    "{\"query\": \"Scene Graph: {scene_graph} Task: {task_description}\",\n",
    " \"response\": \"[\\\"goto(r_10)\\\", \\\"goto(r_6)\\\", \\\"pickup(o_9)\\\", \\\"goto(r_9)\\\", \\\"release(o_9)\\\"]\"}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
